\chapter{Literature review and basic theory}

\section{Hidden Markov Models}
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.7\textwidth,keepaspectratio]{Figures/erg-HMM}
	\caption{Example of a 3-states ergodic HMM}{\cite{thes2011}}
	\label{fig:erg}
\end{figure}
Hidden Markov Models (HMMs) are doubly embedded stochastic processes with a rich underlying statistical structure. Introduced at the end of the 1960s by Baum and colleagues, they have become one of the preferred techniques in speech recognition after the implementation of Baker and Jelinek in 1970s. HMMs have been successfully applied to various other engineering problems in pattern recognition for classification and fraud detection purposes, amongst others \cite{tuto1989} \cite{tool2001} \cite{towa2009} \cite{twop2008}.\\
The type of HMM depends on the possible connections between the states. Thus, an HMM, in which a state can transition to any other state is said ergodic. Other types such as the Left-Right model or Bakis do not allow all possible transitions between the states. Figure \ref{fig:erg} is an illustration of an ergodic HMM of 3 states (for simplicity). The annotations are explained in the next secetion. 

\subsection{HMM parameters specification}
An HMM is fully specified by the following parameters 
\begin{enumerate}
	\item N, the number of distinct states of the model. Together they form the set of individual states \(S = \{S_1, S_2, ..., S_N\}\).
	\item T, the number of observations. A sample observation sequence is denoted as \(O = \{O_1, O_2, ..., O_T\}\).
	\item \(Q = {q_t}\), the set of states with \(q_t\) denoting the current state at time instance, t such that \(q_t \epsilon S\) and \(t = 1, 2, ..., T\).
	\item K, the number of distinct observation symbols per state. 
	\item \(V = \{v_1, v_2, ..., v_K\}\), the feature set of K dimensions.
	\item \(A =  \{a_{ij} \}\), the state transition probabilities. \(a_{ij}\)  denotes probability of transitioning from state \(S_i\) to state \(S_j\).
	\item \(\Phi =   \{ \phi_{j}(k\}\), the probability distribution of observation symbols in state j.
	\item The initial state distribution, \(\pi = {\pi_i}\)
\end{enumerate}
For continuous HMM (CHMM), i.e, HMM with continuous-valued observations, \(\Phi\) consists in a probability distribution function. Many applications have successfully modelled such distributions with mixtures of Gaussian distributions \cite{cont2013} \cite{ches2012}.
As such, \(\phi\) is approximated by a weighted sum of M multivariate Gaussian distributions \(\eta\). For a given, observation sequence, \(\phi\)  and \(\eta\) are therefore given by equations \ref{eq:phi} and \ref{eq:eta},

\begin{align} 
	\phi(O_t) = \sum_{m=1}^M \beta_{jm} \eta(\mu_{jm}, \Sigma_{jm}, O_t), \label{eq:phi} \\
	\eta(\mu, \Sigma, O) = \frac{1}{\sqrt{(2\pi)^K|\Sigma|}}exp(-\frac{1}{2}(O-\mu)'\Sigma^{-1}(O-\mu) \label{eq:eta}
\end{align} 
\begin{align*}
	1 \leq j \leq N; 1 \leq m  \leq M; \beta_{jm} \geq 0; \sum_{m=1}^{M}\beta_{jm} = 1
\end{align*}
where \(\beta_{jm}\) is the mixture composition coefficient; \(\mu_{jm}\), \(\Sigma_{jm}\), respectively the mean vector and covariance matrix of state j; M is the number of mixture components and K is the dimensionality of O. In practice, the log-likelihood of \(\phi(O_t)\) is rather computed to avoid overflow when implemented on a machine. %%TODO: ref this fact

As a summary, the compact specification of a continuous valued observation HMM is defined by \ref{eq:CHMM} and that of a discrete HMM in \ref{eq:DHMM}.
\begin{align} 
	CHMM = \lambda_C = (A, \beta_{jm}, \mu_{jm}, \Sigma_{jm}, \pi) \label{eq:CHMM} \\
	DHMM = \lambda_D = (A, b_j(k), \pi) \label{eq:DHMM}
\end{align}

\subsubsection{Basic assumptions of HMMs theory}
HMM theory is built on three basic assumptions listed below.
\begin{enumerate}
\label{con:markov} \item \textit{The Markov assumption}: HMM assumes that the probability of being in the current	at any instance of time t, is uniquely dependent on the previous state, at time, t + 1. More specifically, \(a_{ij} = P[q_t = S_j|q_{t+1}=S_i]\). This assumption makes it unsuitable for long-range correlation capturing applications.

\item \textit{The stationary assumption}: Furthermore, HMM state transition probabilities are assumed to be time-independent. Thus, the transition probabilities of two distinct time, \(t_1\) and \(t_2\) are identical, \(P[q_{t_1} = S_j|q_{t_1 -1} = S_i] = P[q{t_2}=S_i|q_{t_2-1} = S_i]\). HMMs can therefore effectively model mechanisms with stationary observations.

\item \textit{The output/observation independence assumption}: The current observation also known as emission symbol is statistically independent of the previous observations. It is "emitted" only by the current state, \(P[O|q_1, q_2, ..., q_T, \lambda]=\prod_{t=1}^{T}P[O_t|q_t, \lambda]\).
\end{enumerate} 
The three assumptions make an HMM a relatively simple graphical model to implemented. This simplicity naturally comes with some limitations in modelling more complex problems which, however, may be modelled with higher-order HMMs. %%TODO: look for ref for this fact
Furthermore, the three assumptions are very similar to those of a Markov chain. This is because the stochastic process of an HMM pertaining to the hidden states can be reduced to a Markov chain. In fact, an HMM is an extension of a Markov Chain. The essential difference between the two is that, with the former, there is no a one-to-one mapping between the states and the observation symbols \cite{biol1998}.

\subsection{The three basics problems for HMM design}
In \cite{tuto1989}, Lawrence argued that an HMM design needs to answer three basic problems. They are the \textit{training problem}, the \textit{evaluation problem}, and the \textit{decoding problem}. Each problem and its solution are discussed in greater details next. 

\subsubsection{The evaluation problem}
The evaluation problem is about answering this question:
\textit{Given the observation sequence \(O = O_1O_2O_T\), and a model \(\lambda\), how do we efficiently compute \(P(O|\lambda\), the probability of the observation sequence?} \cite{tuto1989} \cite{ches2012} \cite{cont2013}.
The naive answer to this question is simply computing the \(P(O|\lambda)\) according to equation \ref{eq:P}:
\begin{align}
	P(O|\lambda) = \sum_{q_1}^{q_T}\pi_{q_1}b_{q_1}(O_1)a_{q_1q_2}b_{q_2}(O_2)...a_{q_{T-1}q_T}(O)) \label{eq:P}
\end{align}
This approach has two issues, it is not only, computationally to
o expensive because of the exponential complexity with respect to T, but also, intractable for very long sequence. In practice, \(P(O|\lambda)\) is computed by an algorithm called \textit{forward-backward} procedure, which is a more efficient method.

\subsubsection{The decoding problem}
 The decoding problem can be reduced to this interrogation: \textit{Given the observation sequence \(O = O_1O_2O_T\), and the model \(\lambda\), how do we choose a corresponding state sequence \(Q = q_1q_2...Q_T\) which is optimal in some meaningful sense i.e, best "explains" the observations?}
 Simply put, this problem is about deciphering the most likely hidden states that emitted the visible observation sequences.
 This is done dynamically using the Viterbi algorithm. %%TODO: cite viterbi and explain further if need be
 
\subsubsection{The training problem}
Given the model, \(\lambda\), the training problem raises the following question: \textit{how do we adjust the model parameters \(\lambda\) to maximise the \(P(O|\lambda)\), the probability of the probability of the observation sequence?}
This problem is usually solved by iterative learning algorithms called expectation-maximisation. Examples of this algorithms are Baum-Welch method or any gradient based method. \cite{tuto1989} \cite{tool2001} \cite{biol1998}
\\When using Baum-Welch algorithm, the parameters are initialised by guesses then re-estimated iteratively to find the parameters with maximum likelihood.
This method is vulnerable to local maxima issues. To avoid such cases, it is advised to run it multiple times with different initial values in order to keep the estimation with the highest likelihood value.

\section{Dimensionality reduction}
Dimensionality or dimension reduction is used pattern recognition, machine learning and statistics to find the most compact representation of the dataset by removing redundant and irrelevant information \cite{effi2016}.
It is achieved by extracting principal features, i.e, feature extracting or by selecting the most relevant subset of the initial feature vector, i.e, feature selection, using a supervised or an unsupervised approach.

\subsection{Motivations for dimensionality reduction}
When building a model, the need for dimensionality is supported by several reasons. Some of the important ones are presented in three points.
Firstly, by reducing the feature space's dimension, we can build a model with higher quality\cite{newh2016}.
In most classification problems, the feature domains contain variables with very little to no information for the purpose at hand.
Thus, removing these features reduces the complexity of the problem which can in return, increases the model's accuracy.

Secondly, working with hundreds to thousands of features can be diffult to conceptualise and visualise. By using dimensionality reduction, we can better understand the model and present it to others by comprehensive visualisation\cite{newh2016}.

The third reason is about efficiency in terms of computational time and storage. In general, pattern recognition and machine learning algorithms computationally intensive. Besides, storage capacity is limited in some engineering applications such as embedded systems. So, solving the problem only with the relevant features can alleviate these two problems. Consequently, the computation speed of the model can be increased with dimensionality reduction \cite{hybr2011}.

Various dimensionality reduction have been developed in literature \cite{sima2013} \cite{effi2016} \cite{hybr2011} \cite{newh2016}, 
the next section will present a handful of the ones used in the present work. 


\subsection{Feature selection: filters and wrappers}
Filters and wrappers are two major categories of feature selection methods \cite{effi2016} \cite{newh2016} \cite{hybr2011} \cite{sima2013}. %%Redraw or crop imagines from original paper
The structure of both methods are illustrated by figure \ref{fig:filt} and figure \ref{fig:wrap}, respectively.
\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth,keepaspectratio]{Figures/filter}
	\caption{The procedure of filters in dimensionality reduction}{\cite{hybr2011}}
	\label{fig:filt}
\end{figure}
\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth,keepaspectratio]{Figures/wrapper}
	\caption{The procedure of wrappers in dimensionality reduction}{\cite{hybr2011}}
	\label{fig:wrap}
\end{figure}
They both require a mechanism for generating a subset of the original feature set and a stopping criterion, which can be a simple distance measure or a more sophisticated separability measure between the features. However, wrappers identify the best feature subset using a learning algorithm usually by fully searching the feature space, whereas, filters use a simple measurement metric based on mutual information, correlation and other distance criteria \cite{hybr2011} \cite{sima2013}.
As a result of the two different approaches, filters are fast and do not guarantee optimal classification accuracy. On the other hand, wrappers give accurate prediction results but are very slow.
Both approaches are often combined to build effective and efficient feature selection methods. One such approach is shown in figure \ref{fig:filt-wrap}. 
\begin{figure}[ht!]
	\centering
	\includegraphics[width=.8\textwidth,keepaspectratio]{Figures/filter-wrapper}
	\caption{The procedure of hybrid filter-wrapper in dimensionality reduction}{\cite{hybr2011}}
	\label{fig:filt-wrap}
\end{figure}
In this approach, the filters are used as a preliminary stage to discard irrelevant features. A feature is deemed irrelevant if it cannot discriminate between the different classes or if it contains redundant information \cite{hybr2011}.
The output of the filter stage is a smaller set of relevant features which is fed into the wrapper to find the optimal final subset of features. Thus, the filter stage effectively reduces the search time of the wrapper. 
In classification problems, using the separability index matrix to determine the classification content or degree of the feature generally results in very good results \cite{sima2013}. 
The next section gives a particular attention to this approach.

\subsubsection{Feature ranking using separability index matrix}
In this section, we look into a systematic approach to determine the 'classifiability of a feature' as present in \cite{sima2013}.
In their paper, Jeong-Su, Sang Wan Lee and Zeungnam Bien, proposed a new criterion called separability index matrix to identify features that can discriminate between the different classes of a classification problem.
The important concepts of this method are here defined together with their significance.

\begin{enumerate}
	\item \textbf{Separability Degree Matrix (SDM)}\\\\
	\(SDM_k\) is a CxC symmetric matrix of the separability measures between the different classes of classification problem of C distinct classes given a particular feature \(x_k\).
	It is defined in \ref{eq:sdm}
\subitem \[SDM_k = [J(w_i, w_j; {x_k})] \label{eq:sdm}\]	
\subitem where \(J(w_i, w_j; {x_k})\) denotes the separability or the distance value between class \(w_i\)  and class \(w_j\) when the criterion function \(J(\textbf{.})\) such as mahalanobis \cite{maha2013} or eucludian, is applied to the feature \(x_k\) \cite{sima2013}. 
	\(SDM_k\) is symmetric matrix with zero diagonal values, because for a given feature \(x_k\), \(J(w_i, w_j; {x_k}) = J(w_j, w_i; {x_k})\) and \(J(w_i, w_i; {x_k}) = 0\).
	This observation can be exploited  to half the computation required to calculate \(SDM_v = {SDM_1, SDM_2, ..., SDM_N}\), the set of  all \(SDM\) for the feature set of size N.
	 
	\item \textbf{Separability Index Matrix (SIM)}\\\\
	\(SIM_k = {c_{ij}}\) is a CxC matrix of binary values {0, 1}. if \(c_ij=0\), then the classes \(w_i\) and \(w_j\) are not separable by the feature \(x_k\).\\
	\(SIM_k\) is obtained by applying an threshold function to \(SDM_k\).
	For instance, 
	\begin{align*}
		SIM_k(i, j) = 0 \quad if  \quad SDM_k(i, j) < SDM_{avg}(i, j)\\
		SIM_k(i,j) = 1 \quad if  \quad SDM_k(i,j) \geq SDM_{avg}
	\end{align*}
	where \(SDM_{avg}\) is the element-wise average of \(SDM_v\).
	\(SIM_k\) is significant because it can be used to systematically determine the irrelevant features, i.e, features whose \(SIM_k = 0\), and/or redundant ones. \cite{sima2013}
	
	
	\item \textbf{Classifiability: \(G(x_k)\)} \label{con:class}\\\\
	Although, conventional distance criterion reveal the separability of the feature distribution, \cite{sima2013}
	 we are often more interested in, how effectively can a particular feature distinguish one class from another. This information is denoted \(G(x_k)\), the ' classifiability of a the feature \(x_k\) '.
	 \(G(x_k)\) is computed by \ref{eq:G}.
	 \begin{align}
	 	G(x_k) = \sum_{i = 1}^{C}\sum_{j = 1}^{C}(SIM_k*WM_k) \label{eq:G}
	 	\\ with \quad WM_k = SIM_k / \sum_{i=1}^{N}SIM_i
	 \end{align}
	 where * and / denote respectively element wise matrix multiplication and division.
\end{enumerate}

This method can be used to effectively and efficiently rank the features in a classification for subset selection or a preliminary step in a hybrid filter-wrapper feature selection solution.  

\subsection{Feature extraction}
In this work, two distinct feature extraction tools were explored namely, the linear discriminant analysis and the principal component analysis.
Each technique is further explained in the next two subsections.
 
\subsubsection{Linear discriminant analysis}
Linear discriminant analysis maps a K-dimensional dataset, \(O \in \Bbb R^{K}\) into a subspace that maximizes the class between-scatter with regards to the class within-scatter \cite{prtool}.
In order words, LDA finds the linear transformation - of the dataset into a lower dimensional space - that maximises the between class discrimination \cite{lda2015}
It is a well-received dimensionality reduction tool in the computer vision and pattern recognition community.\cite{lda2015}.

 LDA is a supervised dimensionality reduction method, it does require the dataset to be labeled. On the other hand, PCA, the next feature extraction method is unsupervised method \cite{prtool}.
 
\subsubsection{Principal component analysis}
 Principal component analysis (PCA) is an orthogonal linear transformation of a dataset of T K-dimensional observations, \(O \in \Bbb R^{T\times K}\), into a so-called PC-space defined by the principal components (PC) \cite{dime2017}.
 Naturally, the dimension of the new space is smaller or equal to K, the original set's dimensionality. The components are ordered according to the variance of the features of the observation set. The principal components represent the eigenvectors of the covariance matrices of the features and the eigenvalues are measures of the features' variances. Thus, the first component is the feature with the highest variance \cite{dime2017}.
 In effect, PCA not only transforms the original observation set into a different space but, it also ranks the features according to their variability. It can therefore be used as an unsupervised dimensionality reduction technique by simply discarding the features with low variance after finding the principal components.