\chapter{Results}

\section{Experiement 1: The necessity of combining the front and back IMU sensor measurements}

\subsection{Aim of the experiement}
The objective of this experiment is to determine whether or not it is necessary to consider both the front and back IMU measurements when predicting only the front or back footfalls. Thus, the following hypothesis was constructed:\\
\textbf{\textit{When predicting only the front or back footfalls of the dog, building the model with the aggregated front and back IMU measurements can improve its performance.}}

\subsection{Experiment apparatus}
To perform this experiment, the following materials are required:
\begin{itemize}
	\item \(\lambda_f\) and \(\lambda_b\), two continuous Hidden Markov Models for the front and back limbs respectively specified by \(\lambda_f = (A_f, \beta_{jm_f}, \mu_{jm_f}, \Sigma_{jm_f}, \pi_f)\) and \(\lambda_b = (A_b, \beta_{jm_b}, \mu_{jm_b}, \Sigma_{jm_b}, \pi_b)\).
	\item At least four data samples: two training sets and two testing sets for the two models, where each contains both the front and back IMU measurements.
	\item A measure to evaluate the performance of the CHMM model.
	\item Finally, a way to visualise the results of the experiments
\end{itemize}

\subsection{Experiment procedure}
To verify our hypothesis, similar experiments to predict the back and front footfalls of the dogs were performed. Further details are given next.\\\\
First of all, two front and back models: \(\lambda_{f_{combined}}\) and \(\lambda_{b_{combined}}\) were constructed by using the combined IMU data from two sensor sets.\\
Furthermore, two other front and back models: \(\lambda_{f_{just-front}}\) and \(\lambda_{b_{just-back}}\) were built using only the front IMU and back IMU measurements, respectively. \\Thus, we have two couples of models to be compared: \(( \lambda_{f_{combined}} vs. \lambda_{f_{just-front}})\) and \((\lambda_{b_{combined}} vs. \lambda_{b_{just-back}})\) \\\\
Secondly, the models to be compared were trained using the same training dataset, where, the back or front IMU measurements were removed for \(\lambda_{f_{just-front}}\) and \(\lambda_{b_{just-back}}\), respectively. \\\\
Finally, the models were tested with their corresponding training datasets. This was done purposefully since the relative comparison of the models with 'combined' and 'separate' IMU data is the subject matter, not the individual performances.\\\\
Iteratively, the experiments was performed while varying the size of training dataset. The state decoding accuracies and the corresponding log-likelihood were recorded. They are presented in the following sub-section.

\subsection{Experiment results}
The results of the current experiment are presented in \ref{fig:front-comb-acc}, \ref{fig:front-comb-log}, \ref{fig:back-comb-acc} and \ref{fig:back-comb-log}. In the same order, they represent the front footfalls decoding accuracy, the front sequence log-likehood,  back footfalls decoding accuracy and the back sequence log-likehood.

\begin{figure}[ht!]
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/front-comb-acc}
	\caption{Front footfalls prediction accuracy with both IMUs vs with only the front IMU}
	\label{fig:front-comb-acc}
\end{figure}

\begin{figure}[ht!]
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/front-comb-log}
	\caption{Front footfalls prediction log-likelihood with both IMUs vs with only the front IMU}
	\label{fig:front-comb-log}
\end{figure}

\begin{figure}[ht!]
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/back-comb-acc}
	\caption{Back footfalls prediction accuracy with both IMUs vs with only the front IMU}
	\label{fig:back-comb-acc}
\end{figure}

\begin{figure}[ht!]
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/back-comb-log}
	\caption{Back footfalls prediction log-likelihood with both IMUs vs with only the front IMU}
	\label{fig:back-comb-log}
\end{figure}


\subsection{Analysis and discussion of results}

Let's proceed by analysing the front and back footfalls separately before putting them together with conclusive thoughts.

\subsubsection{Front footfalls}
The experiment revealed that for both the combined and just the front IMU data, the classification accuracy increases with the increase in the training data size as testified by \ref{fig:front-comb-acc}. Overall, building the model with just front sensor measurements achieves better accuracy except with about 3000 and 4500 observation samples.\\
The difference in performance is more significant with smaller data sizes. For instance, with 539 samples, the combination resulted in a very poor performance, below 50\% accuracy, whereas, taking just the front measurements resulted in about 63\%. This is at least 35\% improvement.
This difference is due to the dimensionality of the observation sequences as demonstrated in \ref{exp:feat-size}. With just the front measurements, we are dealing with 9 features against 18 features when the front and back measurements are considered.
After about 3000 samples, the two models prediction accuracies become comparable. This is because, there is sufficiently enough training data to cater for the high dimensionality.\\\\

The ability of the two models to recognise the IMU measurements generated by the dog are very comparable from 1000 samples onwards as shown in \ref{fig:front-comb-log}. Before 1000 samples, the model with just the front sensor measurements gives a higher likelihood value, constantly close to zero. This confirms the trend in the prediction accuracy.

\subsubsection{Back footfalls}

The back footfalls prediction with just the back sensor data and both the front and back sensor data show similar results to the front footfall case. However, the model with the combined sensors measurements catches up quicker at around 1000 samples. It even shows slighly better performance in accuracy after 2500 samples.\\\\
The common observation to both the front and back footfalls, is the following. Only, the front and the back sensor measurements can better predict or recognise the dog's front and the back footfalls respectively, when the available observation sequence is relatively small, below 1000.
This finding is advantageous for several reasons. In fact, this realisation can be used to reduce the dimension of the data from 18 down to 9 when dealing with a small dataset. On one hand, this is not only good in terms of the model's accuracy. On the other hand, when dealing with just the front or the back legs, the required logistics such as the memory size, the unnecessary back or front sensors themselves. 

\subsection{Conclusions and recommendations of the experiment}
In light of the finding above, it can be concluded that: using all the front and back IMU measumrents to predict just the front and back footfalls does not necessarily improves the performance. It degrades the performance when there is not enough training data because of the high dimensionality.
The formulated hypothesis is therefore rejected based on this experiment.\\
An improvement to this experiment would be to explore how the dimensionality reduction of the different datasets would influence the outcome. Moreover, a futher investigation how well can the back sensors measurement be used to predict the front footfalls. Similarly, how well can the front sensors be used to predict the back footfalls.

\section{Experiement 2: The effect of a CHMM' observation dimensionality on its performance} \label{exp:feat-size}

\subsection{Aim of the experiement}
The aim of this experiement is to investigate how the number of features impacts the accuracy of a Hidden Markov Model with continuous emission symbols (CHMM), in the abscence of enough training data. Thus, the hypothesis under investigation is:\\
\textbf{\textit{In the absence of enough training data, a CHMM with observations of high dimensionality performs poorly.}}

\subsection{Experiment apparatus}
To perform this expereiment, the following materials are required:
\begin{itemize}
	\item \(\lambda\), a continuous Hidden Markov Model specified by \(\lambda = (A, \beta_{jm}, \mu_{jm}, \Sigma_{jm}, \pi)\).
	\item At least two sample data sets for training the model and testing it.
	\item A criterion to rank and select subsets of features.
	\item A measure to evaluate the performance of the CHMM model.
	\item Finally, a way to visualise the results of the experiments
\end{itemize}

\subsection{Experiment procedure}
The expreriment was performed with just the back limps by following the steps listed below:
\begin{enumerate}
	\item Step 1 - Preliminary data pre-processing: This step consisted in the data pre-processing as described in \ref{sec:pre-proc}.
	\item Step 2 - Partitioning data into training and test sets: Here, the dataset was randomly sampled into training and test sets. The training set was made relatively small, it was a sequence of 539 observations. 
	\item Step 3 -  Feature ranking: The features were ranked using separability of index \ref{sec:rank}. 
	\item Step 4 - Constructing and training the models: 18 different CHMM models, \(\lambda = {\lambda_i} = \lambda_1, \lambda_2, ..., \lambda_18\) were built and trained using the first i features of the same training dataset.
	\item Step 5 - Testing and evaluation the models: After training the models, they were tested with the same test dataset with the correct feature subsets and their performance were evaluated in terms of decoding accuracy and log-likelihood value. 
	\item The different accuracies and the sequence log-likelihood values were finally ploted as a function of the observation dimensionality. Moreover, the observations were grouped based on the corresponding hidden state sequence and scattered in a 2-dimensional principal component space. This is to compare the decoded states against the ground-truth.
\end{enumerate}

\subsection{Experiment results}
The results of the experiments are presented in figure \ref{fig:dim-acc}, \ref{fig:dim-log}, \ref{fig:gt-5dim}, \ref{fig:es-5dim}, \ref{fig:gt-18dim}, \ref{fig:es-18dim}.\\
Figure \ref{fig:dim-acc} and \ref{fig:dim-log} respectively show how the hidden state decoding accuracy and the test sequence log-likelihood estimated by the CHMM model varies as the the number of features considered increases.\\
Figure \ref{fig:gt-5dim} and \ref{fig:es-5dim} are visualisations of the 5-dimensional observation sequence grouped respectively, according to the actual state sequence and the decoded state sequence. Figure \ref{fig:gt-18dim} and \ref{fig:es-18dim}, are for an 18-dimensional observation.
5 and 18 dimensions were presented because they resulted in the two extreme prediction accuracies. The results for other dimensions may be found in the appendices, %%TODO: ref appendices
\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/dimensionality-effect-acc}
	\caption{The effect of CHMM's observation dimensionality the state sequence decoding accuracy}
	\label{fig:dim-acc}
\end{figure}

\begin{figure}[ht!]
	\includegraphics{Figures/dimensionality-effect-log}
	\caption{The effect of CHMM's observation dimensionality the log-likelihood}
	\label{fig:dim-log}
\end{figure}


\begin{figure}[ht!]
	\includegraphics{Figures/ground-truth-scatter-with-5-features}
	\caption{Scatter plot of  5-dimensional observations grouped per state based on ground-truth state sequence}
	\label{fig:gt-5dim}
\end{figure}
\begin{figure}[ht!]
	\includegraphics{Figures/estimation-scatter-with-5-features}
	\caption{Scatter plot of 5-dimensional observations grouped per state based on estimated state sequence}
	\label{fig:es-5dim}
\end{figure}

\begin{figure}[ht!]
	\includegraphics{Figures/ground-truth-scatter-with-18-features}
	\caption{Scatter plot of 18-dimensional observations grouped per state based on ground-truth state sequence}
	\label{fig:gt-18dim}
\end{figure}

\begin{figure}[ht!]
	\includegraphics{Figures/estimation-scatter-with-18-features}
	\caption{Scatter plot of 18-dimensional observations grouped per state based on estimated state sequence}
	\label{fig:es-18dim}
\end{figure}

\subsection{Analysis and discussion of results}

The CHMM model's state decoding accuracy in \ref{fig:dim-acc} increases from 1 feature and reaches its maximum performance at about 73\% accuracy with 5 features. After 5 features, the accuracy generally declines to about just 40\% accuracy with all 18 features. The decline is however not consistent, for example, there is a local peak at 13 and 17 features.
By considering the performance with initial feature number, i.e, 18 and the highest accuracy, a performance increase of up to 78\% was obtained by reducing the feature set's dimension by about 72\% its initial size.
In addition, the sequence log-likehood value is close to zero from 1 to 8 features. It then slowly declining - in overall - from 9 to 15 features and suddenly gets to its lowest value at 18 dimensions.
The log-likelihood being a measure of the probability that the test sequence was generated by the model, it is clear that the CHMM model better recognises test sequence with smaller feature sizes.\\
Thus, the smaller the feature size, the better the CHMM performance with a relatively small training dataset. The model is both better at decoding the hidden states as well as recognising IMU measurements generated by the dog's gait mechanism.
This fact is further shown by \ref{fig:gt-5dim}, \ref{fig:es-5dim}, \ref{fig:gt-18dim} and \ref{fig:es-18dim}. With 5 features, the model correctly attributes IMU measurements to the correct footfalls with at the exception of state 4 (both legs UP) which were mostly classified as state 3 (left leg DOWN and right leg UP). In opposite, with 18 features, the HMM model predicted wrongly that the dog remained in state 3 for a significant proportion of the 539 sequences. In fact, with 18 dimensions, the model is not two times better than taking a random guess, i.e, \(0.4 < 2\times\frac{1}{4}\) to choose one of the four states.

\subsection{Conclusions and recommendations of the experiment}
Based on the above results and discussions, it can be concluded that in the absence of enough training data, a CHMM performs very poorly. This validates the hypothesis formulated. It is therefore necessary to further explore the effect of dimensionality reduction with more robust techniques. This investigation is performed in the next experiment with various dimensionality reduction technique and varying data sizes.

\section{Experiement 3: The effect of dimensionality reduction on CHMM's performance}  \label{exp:dim}

\subsection{Aim of the experiement}
The aim of this experiment is to investigate the effect of dimensionality reduction on the performance of a continuous Hidden Markov Model (CHMM).
The hypothesis under investigation is therefore the following:
\textbf{\textit{Dimension reduction can cause an increase in a CHMM performance when there is not enough training data.}}
Thus, the performances of the CHMM with and without dimensionality reduction are compared to test the hypothesis.

\subsection{Experiment apparatus}
The assets needed to perform the experiment are listed below.
\begin{itemize}
	\item \(\lambda\), a continuous Hidden Markov Model specified by \(\lambda = (A, \beta_{jm}, \mu_{jm}, \Sigma_{jm}, \pi)\).
	\item Dimensionality reduction methods. Two wrapper and two filter methods were considered. The wrapper methods were Principle Component Analysis (PCA), Linear Discriminant Analysis (LDA). The two filters methods were feature ranking with similarity index %%TODO cross-ref
	and a combination of forward feature selection and similarity index.
	\item A Performance metric. The metric used were the hidden state decoding accuracy and the log-likelihood of the EM algorithm.
\end{itemize}

\subsection{Experiment procedure}
The experiment was performed as follows.
First, the dataset was partition into two different set for training and testing using random sampling.
Using the same traing dataset five different models were built and trained, \(\Lambda = (\lambda_{No Reduction}, \lambda_{PCA}, \lambda_{LDA}, \lambda_{SI}, \lambda_{SI-forward})\).
Then the different models were tested with the same test dataset and the prediction accuracy as well as the EM algorithm loglikehood were recorded.
The training and the testing were repeated while varying the proportion of training set used from \(10\%\) to \(90\%\) of the total dataset.
The prediction accuracies and the EM algorithm likelihoods were finally plotted as a function of the training data size for each model.
These findings are presented in the figures \ref{fig:size-acc} and \ref{fig:size-log}. 

\subsection{Experiment results}
Firstly, figure \ref{fig:size-acc} how the performances of the five CHMMs compare against each other as the training data size increases.
Secondly, the loglikelihoods presented in \ref{fig:size-log} show how effectively can each model recognise an observation sequence generated by the underlying mechanism.

\begin{figure}[ht!]
	\includegraphics{Figures/size-acc-2}
	\caption{The effect of training datasize on the prediction accuracy}
	\label{fig:size-acc}
\end{figure}

\begin{figure}[ht!]
	\includegraphics{Figures/size-log-2}
	\caption{The effect of training datasize on the log likelihood}
	\label{fig:size-log}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics{Figures/bias-var}
	\caption{Bias-Variance tradeoff analysis}
	\label{fig:bias-var}
\end{figure}



\subsection{Analysis of results}

\subsection{Conclusions and recommendations of the experiment}

\section{Experiement 4: Motion or action recognition}  \label{exp:motion}

\subsection{Aim of the experiement}
This experiment is about using IMU based HMM to identfy a particular action of the dog. The three actions considered are: running, walking and trotting.
The hypothesis under investigation is therefore the following:
\textbf{\textit{IMU based HMMs can be used to successfully perform motion recognition.}}

\subsection{Experiment apparatus}
The assets needed to perform the experiment are listed below.
\begin{itemize}
	\item \(\lambda\), a continuous Hidden Markov Model specified by \(\lambda = (A, \beta_{jm}, \mu_{jm}, \Sigma_{jm}, \pi)\).
	\item Dimensionality reduction methods. Two wrapper and two filter methods were considered. The wrapper methods were Principle Component Analysis (PCA), Linear Discriminant Analysis (LDA). The two filters methods were feature ranking with similarity index %%TODO cross-ref
	and a combination of forward feature selection and similarity index.
	\item A Performance metric. The metric used were the hidden state decoding accuracy and the log-likelihood of the EM algorithm.
\end{itemize}

\subsection{Experiment procedure}
The experiment was performed as follows.
First, the dataset was partition into two different set for training and testing using random sampling.
Using the same traing dataset five different models were built and trained, \(\Lambda = (\lambda_{No Reduction}, \lambda_{PCA}, \lambda_{LDA}, \lambda_{SI}, \lambda_{SI-forward})\).
Then the different models were tested with the same test dataset and the prediction accuracy as well as the EM algorithm loglikehood were recorded.
The training and the testing were repeated while varying the proportion of training set used from \(10\%\) to \(90\%\) of the total dataset.
The prediction accuracies and the EM algorithm likelihoods were finally plotted as a function of the training data size for each model.
These findings are presented in the figures \ref{fig:size-acc} and \ref{fig:size-log}. 

\subsection{Experiment results}
Firstly, figure \ref{fig:size-acc} how the performances of the five CHMMs compare against each other as the training data size increases.
Secondly, the loglikelihoods presented in \ref{fig:size-log} show how effectively can each model recognise an observation sequence generated by the underlying mechanism.


\subsection{Analysis of results}

\subsection{Conclusions and recommendations of the experiment}


\section{Experiement 5: Comparing the back and front gait of the dog}  \label{exp:front-back}

\subsection{Aim of the experiement}
The present experiment is very similar to motion recognition experiment in \ref{exp:motion} in nature but, it differs in purpose. Here, the aim is to study the similarities and differences between the front and back limb movements of a dog, given a specific action. The dog's run, walk and trot where investigated.
The hypothesis below was therefore formulated.
\textbf{\textit{The front and back limbs of a dog exhibit gait patterns.}}

\subsection{Experiment apparatus}
The assets needed to perform the experiment are listed below.
\begin{itemize}
	\item \(\lambda\), a continuous Hidden Markov Model specified by \(\lambda = (A, \beta_{jm}, \mu_{jm}, \Sigma_{jm}, \pi)\).
	\item Dimensionality reduction methods. Two wrapper and two filter methods were considered. The wrapper methods were Principle Component Analysis (PCA), Linear Discriminant Analysis (LDA). The two filters methods were feature ranking with similarity index %%TODO cross-ref
	and a combination of forward feature selection and similarity index.
	\item A Performance metric. The metric used were the hidden state decoding accuracy and the log-likelihood of the EM algorithm.
\end{itemize}

\subsection{Experiment procedure}
The experiment was performed as follows.
First, the dataset was partition into two different set for training and testing using random sampling.
Using the same traing dataset five different models were built and trained, \(\Lambda = (\lambda_{No Reduction}, \lambda_{PCA}, \lambda_{LDA}, \lambda_{SI}, \lambda_{SI-forward})\).
Then the different models were tested with the same test dataset and the prediction accuracy as well as the EM algorithm loglikehood were recorded.
The training and the testing were repeated while varying the proportion of training set used from \(10\%\) to \(90\%\) of the total dataset.
The prediction accuracies and the EM algorithm likelihoods were finally plotted as a function of the training data size for each model.
These findings are presented in the figures \ref{fig:size-acc} and \ref{fig:size-log}. 

\subsection{Experiment results}
Firstly, figure \ref{fig:size-acc} how the performances of the five CHMMs compare against each other as the training data size increases.
Secondly, the loglikelihoods presented in \ref{fig:size-log} show how effectively can each model recognise an observation sequence generated by the underlying mechanism.

\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls (\%)} &  \textbf{Back footfalls (\%)}\\ 
		\hline
		Front footfalls & 88.9210 & 17.4678\\ 
		\hline
		Back footfalls & 14.3545 & 89.5708 \\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction accuracies for running}
	\label{tab:front-back-run-acc}
\end{table}

\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls} &  \textbf{Back footfalls}\\ 
		\hline
		Front footfalls &  8.4012x\(10^{-4}\) & 4.2653x\(10^{-4}\)\\ 
		\hline
		Back footfalls & 7.9955x\(10^{-4}\) & 8.6471x\(10^{-4}\) \\ 
		\hline	   	
	\end{tabular}
	\caption{Log-likelihood for running}
	\label{tab:front-back-run-log}
\end{table}


\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls} &  \textbf{Back footfalls}\\ 
		\hline
		Front footfalls &  100.0000x\(10^{-4}\) & 55.8629x\(10^{-4}\)\\ 
		\hline
		Back footfalls & 55.7803x\(10^{-4}\) & 100.0000x\(10^{-4}\) \\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction accuracies for walking}
	\label{tab:front-back-walk-acc}
\end{table}

\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls(\%)} &  \textbf{Back footfalls(\%)}\\ 
		\hline
		Front footfalls & 2.6346x\(10^{-4}\) & 2.6346x\(10^{-4}\)\\ 
		\hline
		Back footfalls & 2.6025x\(10^{-4}\) & 2.6025x\(10^{-4}\)\\ 
		\hline	   	
	\end{tabular}
	\caption{Log-likelihood for walking}
	\label{tab:front-back-walk-log}
\end{table}


\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls (\%)} &  \textbf{Back footfalls(\%)}\\ 
		\hline
		Front footfalls &  100.0000 & 74.6082\\ 
		\hline
		Back footfalls & 74.6082 & 100.0000 \\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction accuracies for trotting}
	\label{tab:front-back-trot-acc}
\end{table}


\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls} &  \textbf{Back footfalls}\\ 
		\hline
		Front footfalls & 7.0234x\(10^{-4}\) & 7.0234x\(10^{-4}\)\\ 
		\hline
		Back footfalls & 6.8193x\(10^{-4}\) & 6.8193x\(10^{-4}\)\\ 
		\hline	   	
	\end{tabular}
	\caption{Log-likelihood for trotting}
	\label{tab:front-back-trot-log}
\end{table}

\subsection{Analysis of results}

\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls (\%)} &  \textbf{Back footfalls (\%)}\\ 
		\hline
		Front footfalls & 1 & -1\\ 
		\hline
		Back footfalls & -1 & 1 \\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction accuracies correlation for running}
	\label{tab:front-back-run-acc-corr}
\end{table}

\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls} &  \textbf{Back footfalls}\\ 
		\hline
		Front footfalls & 1 & -1\\ 
		\hline
		Back footfalls & -1 & 1 \\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction log-likelihood correlation for running}
	\label{tab:front-back-run-log-corr}
\end{table}


\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls} &  \textbf{Back footfalls}\\ 
		\hline
		Front footfalls &  1 & -1\\ 
		\hline
		Back footfalls & -1 & 1 \\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction accuracies correction for walking}
	\label{tab:front-back-walk-acc-corr}
\end{table}

\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls(\%)} &  \textbf{Back footfalls(\%)}\\ 
		\hline
		Front footfalls & 1 & 1\\ 
		\hline
		Back footfalls & 1 & 1\\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction log-likelihood correlation for walking}
	\label{tab:front-back-walk-log-corr}
\end{table}


\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls (\%)} &  \textbf{Back footfalls(\%)}\\ 
		\hline
		Front footfalls &  -1 & 1\\ 
		\hline
		Back footfalls & 1 & -1 \\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction accuracies correlation for trotting}
	\label{tab:front-back-trot-acc-corr}
\end{table}


\begin{table}[h!] 
	\centering
	\begin{tabular}{ |c|c|c|} 	
		\hline	
		\textbf{Body part} & \textbf{Front footfalls} &  \textbf{Back footfalls}\\ 
		\hline
		Front footfalls & 1 & 1\\ 
		\hline
		Back footfalls & 1 & 1\\ 
		\hline	   	
	\end{tabular}
	\caption{Prediction log-likelihood correlation for troting}
	\label{tab:front-back-trot-log-corr}
\end{table}

\subsection{Conclusions and recommendations of the experiment}